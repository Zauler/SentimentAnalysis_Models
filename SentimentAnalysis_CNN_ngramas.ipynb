{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional sentiment analysis model with three inputs to the model and one output for sentiment prediction.\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word)>1]\n",
    "    return tokens\n",
    "\n",
    "filename = 'data/txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, is_train):\n",
    "    documents = list()\n",
    "    for filename in listdir(directory):\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        doc = load_doc(path)\n",
    "        tokens = clean_doc(doc)\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "def load_clean_dataset(is_train):\n",
    "    neg = process_docs('data/txt_sentoken/neg', is_train)\n",
    "    pos = process_docs('data/txt_sentoken/pos', is_train)\n",
    "    docs = neg + pos\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train.pkl\n",
      "Saved: test.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load and clear dataset \n",
    "train_docs, ytrain = load_clean_dataset(True)\n",
    "test_docs, ytest = load_clean_dataset(False)\n",
    "# save train/test\n",
    "save_dataset([train_docs,ytrain], \"train.pkl\")\n",
    "save_dataset([test_docs,ytest], \"test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## develop multi-channel model \n",
    "\n",
    "The network uses three distinct inputs, each processed by its own convolutional network with a different kernel size. Each convolutional channel is designed to capture patterns in sequences of words of varying length, grouped according to the kernel size specific to that layer (2, 4 and 8 words, respectively). The combination of features extracted from these different 'n-grams' allows the network to integrate multiple perspectives of the input data to predict a single output result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "def load_dataset(filename):\n",
    "    return load(open(filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we want to control the number of occurrences of the words we will have to make a vocabulary before as in the previous example.\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer(num_words=3000) # if you don't put it is the whole size \n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max([len(s) for s in lines])\n",
    "\n",
    "def mid_length(lines):\n",
    "     total_length = sum(([len(s) for s in lines]))\n",
    "     number_of_lines = len(lines)\n",
    "     return int(total_length / number_of_lines if number_of_lines else 0)\n",
    " \n",
    "def encoded_text(tokenizer, lines, length):\n",
    "    encoded= tokenizer.texts_to_sequences(lines) #encode to integer\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding=\"post\") #fill sequences to the maximum length\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#External models to perform text encoding, we will use both in the examples and evaluate them to see the result, we will also perform the example without using external encoding. In any case the results are better if we do not block them in the training even if we use them.\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vected_lines(tokenizer,lines,min_count):\n",
    "    embedding_index = dict()\n",
    "    model = Word2Vec(lines,min_count=min_count)\n",
    "    #We see the vocabulary created\n",
    "    words = model.wv.index_to_key\n",
    "    vectors = model.wv.vectors\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        coef = np.asanyarray(vectors[i])\n",
    "        embedding_index[word] = coef \n",
    "        \n",
    "    vocab_size = len(tokenizer.word_index)+1\n",
    "    embedding_matix = np.zeros((vocab_size, 100)) #vocabulary and loaded weights\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embedding_index.get(word) #get returns None if it fails\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matix[i] = embedding_vector\n",
    "    return embedding_matix       \n",
    "\n",
    "def gloved_test(tokenizer):\n",
    "    embedding_index = dict()\n",
    "    f = open(\"glove.6B.100d.txt\", mode=\"rt\", encoding=\"utf-8\") #to use this part, you need to have the document with the weights of the glove vectors that you can download on the internet.\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coef = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embedding_index[word] = coef\n",
    "    f.close()\n",
    "    vocab_size = len(tokenizer.word_index)+1\n",
    "    embedding_matix = np.zeros((vocab_size, 100)) #vocabulary and loaded weights\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embedding_index.get(word) #get returns None if it fails\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matix[i] = embedding_vector\n",
    "    return embedding_matix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Input\n",
    "from pickle import load \n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The n-gram model will have 3 channels which will then be merged into 1, a concatenation layer and then a flattening and output.\n",
    "\n",
    "def define_model(length, vocab_size, embeddin_matix=None, embedding_trainable=False):\n",
    "    if embeddin_matix is None:\n",
    "        embeddin_matix = None\n",
    "    else:\n",
    "        embeddin_matix = [embeddin_matix]\n",
    "\n",
    "    # Specify channel 1 for 4-grams\n",
    "    inputs1 = Input(shape= (length,))\n",
    "    embedding1= Embedding(vocab_size, output_dim=100, weights = embeddin_matix, trainable = embedding_trainable)(inputs1)\n",
    "    conv1 = Conv1D(16, 2, activation=\"relu\")(embedding1) #4 viene de los gramas.( o palabras que entran )\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D()(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    \n",
    "    #Specify channel 2 - 6 grammes\n",
    "    inputs2 = Input(shape= (length,))\n",
    "    embedding2= Embedding(vocab_size, output_dim=100, weights = embeddin_matix, trainable = embedding_trainable)(inputs2)\n",
    "    conv2 = Conv1D(32, 4, activation=\"relu\")(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D()(drop2)\n",
    "    flat2 = Flatten()(pool2)    \n",
    "    \n",
    "    \n",
    "    #Specify for channel 3 - 8 grams.\n",
    "    \n",
    "    inputs3 = Input(shape= (length,))\n",
    "    embedding3= Embedding(vocab_size, output_dim=100, weights = embeddin_matix, trainable = embedding_trainable)(inputs3)\n",
    "    conv3 = Conv1D(64, 8, activation=\"relu\")(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D()(drop3)\n",
    "    flat3 = Flatten()(pool3)    \n",
    "\n",
    "    #concatenation\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    \n",
    "    #Flatten\n",
    "    dense1 = Dense(32, activation= \"relu\")(merged)\n",
    "    drop5 = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(16, activation= \"relu\")(drop5)\n",
    "    outputs = Dense(1, activation = \"sigmoid\") (dense2)\n",
    "    model = Model(inputs = [inputs1,inputs2,inputs3], outputs = outputs)\n",
    "    \n",
    "    #compile\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    model.summary()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud m치xima de documentos: 1380\n",
      "Tama침o del vocabulario: 44277 \n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_41 (InputLayer)       [(None, 1380)]               0         []                            \n",
      "                                                                                                  \n",
      " input_42 (InputLayer)       [(None, 1380)]               0         []                            \n",
      "                                                                                                  \n",
      " input_43 (InputLayer)       [(None, 1380)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_40 (Embedding)    (None, 1380, 100)            4427700   ['input_41[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_41 (Embedding)    (None, 1380, 100)            4427700   ['input_42[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_42 (Embedding)    (None, 1380, 100)            4427700   ['input_43[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)          (None, 1379, 16)             3216      ['embedding_40[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_40 (Conv1D)          (None, 1377, 32)             12832     ['embedding_41[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_41 (Conv1D)          (None, 1373, 64)             51264     ['embedding_42[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)        (None, 1379, 16)             0         ['conv1d_39[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_53 (Dropout)        (None, 1377, 32)             0         ['conv1d_40[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_54 (Dropout)        (None, 1373, 64)             0         ['conv1d_41[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_39 (MaxPooli  (None, 689, 16)              0         ['dropout_52[0][0]']          \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling1d_40 (MaxPooli  (None, 688, 32)              0         ['dropout_53[0][0]']          \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling1d_41 (MaxPooli  (None, 686, 64)              0         ['dropout_54[0][0]']          \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " flatten_39 (Flatten)        (None, 11024)                0         ['max_pooling1d_39[0][0]']    \n",
      "                                                                                                  \n",
      " flatten_40 (Flatten)        (None, 22016)                0         ['max_pooling1d_40[0][0]']    \n",
      "                                                                                                  \n",
      " flatten_41 (Flatten)        (None, 43904)                0         ['max_pooling1d_41[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenat  (None, 76944)                0         ['flatten_39[0][0]',          \n",
      " e)                                                                  'flatten_40[0][0]',          \n",
      "                                                                     'flatten_41[0][0]']          \n",
      "                                                                                                  \n",
      " dense_39 (Dense)            (None, 32)                   2462240   ['concatenate_13[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_55 (Dropout)        (None, 32)                   0         ['dense_39[0][0]']            \n",
      "                                                                                                  \n",
      " dense_40 (Dense)            (None, 16)                   528       ['dropout_55[0][0]']          \n",
      "                                                                                                  \n",
      " dense_41 (Dense)            (None, 1)                    17        ['dense_40[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15813197 (60.32 MB)\n",
      "Trainable params: 2530097 (9.65 MB)\n",
      "Non-trainable params: 13283100 (50.67 MB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "180/180 [==============================] - 3s 12ms/step - loss: 0.7057 - accuracy: 0.5617\n",
      "Epoch 2/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.5997 - accuracy: 0.6733\n",
      "Epoch 3/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.4320 - accuracy: 0.8094\n",
      "Epoch 4/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.3095 - accuracy: 0.8750\n",
      "Epoch 5/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.2151 - accuracy: 0.9122\n",
      "Epoch 6/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.1585 - accuracy: 0.9378\n",
      "Epoch 7/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.1108 - accuracy: 0.9639\n",
      "Epoch 8/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0957 - accuracy: 0.9650\n",
      "Epoch 9/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.1073 - accuracy: 0.9617\n",
      "Epoch 10/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.1154 - accuracy: 0.9600\n",
      "Epoch 11/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0642 - accuracy: 0.9778\n",
      "Epoch 12/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0732 - accuracy: 0.9733\n",
      "Epoch 13/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0847 - accuracy: 0.9700\n",
      "Epoch 14/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0790 - accuracy: 0.9689\n",
      "Epoch 15/15\n",
      "180/180 [==============================] - 2s 12ms/step - loss: 0.0687 - accuracy: 0.9772\n"
     ]
    }
   ],
   "source": [
    "# Load the train\n",
    "trainLines, trainLabels = load_dataset(\"train.pkl\")\n",
    "# Create the tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# embedding matrix\n",
    "embedding_matrix = gloved_test(tokenizer) # use glove model with data vectors\n",
    "#embedding_matrix = word2vected_lines(tokenizer,trainLines,min_count=3) #Word2vec model\n",
    "#Calculate maximum length\n",
    "length = max_length(trainLines) #--> This is the initial one\n",
    "#length = mid_length(trainLines)\n",
    "print(\"Longitud m치xima de documentos: %d\" % length)\n",
    "# calculate the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print(\"Tama침o del vocabulario: %d \" % vocab_size)\n",
    "\n",
    "#encoding data\n",
    "trainX= encoded_text(tokenizer, trainLines, length)\n",
    "\n",
    "#define model\n",
    "model = define_model(length, vocab_size,embedding_matrix,embedding_trainable=False) \n",
    "\n",
    "#train the model\n",
    "model.fit([trainX, trainX, trainX], np.array(trainLabels), epochs=15, batch_size=10)\n",
    "\n",
    "#save the model\n",
    "model.save(\"model_ngramas.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the data and the model so that we can see a way to create models and use them in another script, in this case it would not be necessary since we would only have to use these variables.\n",
    "# Load the test data\n",
    "testLines, testLabels = load_dataset(\"test.pkl\")\n",
    "\n",
    "#encoding data\n",
    "testX= encoded_text(tokenizer, testLines, length)\n",
    "\n",
    "#load model\n",
    "model = load_model(\"model_ngramas.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, tokenizer,max_length, model):\n",
    "    line = clean_doc(review)\n",
    "    #Code and fill in\n",
    "    padded = encoded_text(tokenizer,[line], max_length)\n",
    "    #Predict the review\n",
    "    yhat = model.predict([padded,padded,padded], verbose=1)\n",
    "    #Extract the percentage of the given review\n",
    "    porcentaje = yhat[0,0]\n",
    "    if round(porcentaje) == 0:\n",
    "        return (1- porcentaje), \"Negative\"\n",
    "    return porcentaje, \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n",
      "Review: [it's the worts film that I have the pleasure of been watched by me, I don't like this film]\n",
      "Sentiment: Negative (87.530%)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Review: [it's the best movie I have ever seen]\n",
      "Sentiment: Positive (74.097%)\n"
     ]
    }
   ],
   "source": [
    "to_evaluate = \"it's the worts film that I have the pleasure of been watched by me, I don't like this film\"\n",
    "percent, sentiment = predict_sentiment(to_evaluate,tokenizer,length,model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (to_evaluate, sentiment, percent*100))\n",
    "\n",
    "\n",
    "to_evaluate = \"it's the best movie I have ever seen\"\n",
    "percent, sentiment = predict_sentiment(to_evaluate,tokenizer,length,model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (to_evaluate, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without external embebbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Train 100.00\n",
      "Accuracy en Train 84.00\n"
     ]
    }
   ],
   "source": [
    "#Evaluate in train \n",
    "_ , acc = model.evaluate([trainX,trainX,trainX], np.array(trainLabels), verbose=0)\n",
    "print(\"Accuracy en Train %.2f\" % (acc*100) )\n",
    "#Evaluate in test\n",
    "_ , acc = model.evaluate([testX,testX,testX], np.array(testLabels), verbose=0)\n",
    "print(\"Accuracy en Train %.2f\" % (acc*100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Train 99.94\n",
      "Accuracy en Train 82.00\n"
     ]
    }
   ],
   "source": [
    "#Evaluate in train \n",
    "_ , acc = model.evaluate([trainX,trainX,trainX], np.array(trainLabels), verbose=0)\n",
    "print(\"Accuracy en Train %.2f\" % (acc*100) )\n",
    "#Evaluate in test\n",
    "_ , acc = model.evaluate([testX,testX,testX], np.array(testLabels), verbose=0)\n",
    "print(\"Accuracy en Train %.2f\" % (acc*100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en Train 100.00\n",
      "Accuracy en Train 77.00\n"
     ]
    }
   ],
   "source": [
    "#Evaluate in train \n",
    "_ , acc = model.evaluate([trainX,trainX,trainX], np.array(trainLabels), verbose=0)\n",
    "print(\"Accuracy en Train %.2f\" % (acc*100) )\n",
    "#Evaluate in test\n",
    "_ , acc = model.evaluate([testX,testX,testX], np.array(testLabels), verbose=0)\n",
    "print(\"Accuracy en Train %.2f\" % (acc*100) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
